---
title: "MODEL 1 STACKING MODEL"
author: "JUDISMA SALI"
date: "2022-12-16"
output:
  pdf_document: default
  html_document: default
---

```{r}
# Helper packages
library(rsample)   # for creating our train-test splits
library(recipes)   # for minor feature engineering tasks
library(tidyverse) # for filtering 
library(readr)     #load dataset

# Modeling packages
library(ROCR)
library(pROC)
library(h2o)       # for fitting stacked models
h2o.init()
```
```{r}
h2o.init()
```

# MODEL 1 STACKING"

Stacking is a process where the data is transformed, and variables (columns) can be rearranged to act as cases (rows). This is sometimes called hierarchical data.


# LOAD THE REPROCESSED DATASET

Note that we used the reprocessed data of radiomics_complete.csv (*RAD. NORMAL DATA.CSV*) in performing stacking.

Radiomics Dataset 197 Rows (Observations) of 431 Columns (Variables)
Failure.binary: binary property to predict

```{r}
radiomicsdt <- read_csv("RAD. NORMAL DATA.CSV")
View(radiomicsdt)
head(radiomicsdt)
```

# CHECKING FOR NULL AND MISSING VALUES

 The result for checking null and missing values is 0 using *sum(is.n())*. Thus, there is no null and missing values.

```{r}
sum(is.na(radiomicsdt))
```


```{r}
set.seed(123)  # for reproducibility

radiomicsdt<- read_csv("RAD. NORMAL DATA.CSV")
radiomicsdt$Failure.binary=as.factor(radiomicsdt$Failure.binary)


split <- initial_split(radiomicsdt, strata = "Failure.binary")
traindt <- training(split)
testdt <- testing(split)
```


```{r}
# Make sure we have consistent categorical levels
blueprint <- recipe(Failure.binary ~ ., data = traindt) %>%
  step_other(all_nominal(), threshold = 0.005)

# Create training & test sets for h2o
h2o.init()
train_h2o <- prep(blueprint, training = traindt, retain = TRUE) %>%
  juice() %>%
  as.h2o()
test_h2o <- prep(blueprint, training = traindt) %>%
  bake(new_data = testdt) %>%
  as.h2o()

# Get response and feature names
Y <- "Failure.binary"
```


```{r}
X <- setdiff(names(traindt), Y)

# Train & cross-validate a GLM model
best_glm <- h2o.glm(
  x = X, y = Y, training_frame = train_h2o, alpha = 0.1,
  remove_collinear_columns = TRUE, nfolds = 10, fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE, seed = 123
)
```


```{r}
# Train & cross-validate a RF model
best_rf <- h2o.randomForest(
  x = X, y = Y, training_frame = train_h2o, ntrees = 1000, mtries = 20,
  max_depth = 30, min_rows = 1, sample_rate = 0.8, nfolds = 10,
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123, stopping_rounds = 50, stopping_metric = "logloss",
  stopping_tolerance = 0
)
```


```{r}
# Train & cross-validate a GBM model
best_gbm <- h2o.gbm(
  x = X, y = Y, training_frame = train_h2o, ntrees = 1000, learn_rate = 0.01,
  max_depth = 7, min_rows = 5, sample_rate = 0.8, nfolds = 10,
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123, stopping_rounds = 50, stopping_metric = "logloss",
  stopping_tolerance = 0
)
```


```{r}
# Get results from base learners
get_rmse <- function(model) {
  results <- h2o.performance(model, newdata = test_h2o)
  results@metrics$RMSE
}
list(best_glm, best_rf, best_gbm) %>%
  purrr::map_dbl(get_rmse)
## [1] 30024.67 23075.24 20859.92 21391.20
```

```{r}
# Define GBM hyperparameter grid
hyper_grid <- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(0.99, 1),
  sample_rate = c(0.5, 0.75, 1),
  col_sample_rate = c(0.8, 0.9, 1)
)

# Define random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  max_models = 25
)
```


```{r}
# Build random grid search 
random_grid <- h2o.grid(
  algorithm = "gbm", grid_id = "gbm_grid", x = X, y = Y,
  training_frame = train_h2o, hyper_params = hyper_grid,
  search_criteria = search_criteria, ntrees = 20, stopping_metric = "logloss",     
  stopping_rounds = 10, stopping_tolerance = 0, nfolds = 10, 
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123
)
```



```{r}
ensemble_tree <- h2o.stackedEnsemble(
  x = X, y = Y, training_frame = train_h2o, model_id = "ensemble_gbm_grid",
  base_models = random_grid@model_ids, metalearner_algorithm = "gbm",
)
```


```{r}
# Stacked results
h2o.performance(ensemble_tree, newdata = test_h2o)@metrics$RMSE
## [1] 20664.56

data.frame(
  GLM_pred = as.vector(h2o.getFrame(best_glm@model$cross_validation_holdout_predictions_frame_id$name))%>%as.numeric(),
  RF_pred = as.vector(h2o.getFrame(best_rf@model$cross_validation_holdout_predictions_frame_id$name))%>%as.numeric(),
  GBM_pred = as.vector(h2o.getFrame(best_gbm@model$cross_validation_holdout_predictions_frame_id$name))%>%as.numeric()
) %>% cor()
```


```{r}
# Sort results by RMSE
h2o.getGrid(
  grid_id = "gbm_grid", 
  sort_by = "logloss"
)

random_grid_perf <- h2o.getGrid(
  grid_id = "gbm_grid", 
  sort_by = "logloss"
)
```


```{r}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- random_grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.performance(best_model, newdata = test_h2o)
```


```{r}
# Train a stacked ensemble using the GBM grid
ensemble <- h2o.stackedEnsemble(
  x = X, y = Y, training_frame = train_h2o, model_id = "ensemble_gbm_grid",
  base_models = random_grid@model_ids, metalearner_algorithm = "gbm"
)
```


```{r}
# Eval ensemble performance on a test set
h2o.performance(ensemble, newdata = test_h2o)
```


```{r}
# Use AutoML to find a list of candidate models (i.e., leaderboard)
auto_ml <- h2o.automl(
  x = X, y = Y, training_frame = train_h2o, nfolds = 5, 
  max_runtime_secs = 60 * 120, max_models = 10,#max_models=50
  keep_cross_validation_predictions = TRUE, sort_metric = "logloss", seed = 123,
  stopping_rounds = 10, stopping_metric = "logloss", stopping_tolerance = 0
)
```


```{r}
# Assess the leader board; the following truncates the results to show the top 
# and bottom 15 models. You can get the top model with auto_ml@leader
auto_ml@leaderboard %>% 
  as.data.frame() %>%
  dplyr::select(model_id, logloss) %>%
  dplyr::slice(1:25)

```


```{r}
# Compute predicted probabilities on training data
train_h2o=as.h2o(traindt)
m1_prob <- predict(auto_ml@leader, train_h2o, type = "prob")
m1_prob=as.data.frame(m1_prob)[,2]
train_h2o=as.data.frame(train_h2o)
# Compute AUC metrics for cv_model1,2 and 3 
perf1 <- prediction(m1_prob,train_h2o$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")


# Plot ROC curves for cv_model1,2 and 3 
plot(perf1, col = "black", lty = 2)


# ROC plot for training data
roc( train_h2o$Failure.binary ~ m1_prob, plot=TRUE, legacy.axes=FALSE, 
    percent=TRUE, col="black", lwd=2, print.auc=TRUE)


# 
# #Feature Interpretation
# vip(cv_model3, num_features = 20)

# Compute predicted probabilities on training data
test_h2o=as.h2o(testdt)

m2_prob <- predict(auto_ml@leader, test_h2o, type = "prob")

m2_prob=as.data.frame(m2_prob)[,2]

test_h2o=as.data.frame(test_h2o)

# Compute AUC metrics for cv_model1,2 and 3 
perf2 <- prediction(m2_prob,test_h2o$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")


# Plot ROC curves for cv_model1,2 and 3 
plot(perf2, col = "black", lty = 2)


# ROC plot for training data
roc( test_h2o$Failure.binary ~ m2_prob, plot=TRUE, legacy.axes=FALSE, 
    percent=TRUE, col="black", lwd=2, print.auc=TRUE)

```

```{r}
test_h2o=as.h2o(test_h2o)
h2o.permutation_importance_plot(auto_ml@leader,test_h2o,num_of_features = 20)
```



